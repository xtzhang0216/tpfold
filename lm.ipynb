{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pubhome/xtzhang/anaconda3/envs/TMprotein_predict/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG']=\":4096:8\"\n",
    "import typing as T\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import *\n",
    "import torch\n",
    "from esm.esmfold.v1 import esmfold\n",
    "import argparse\n",
    "import json\n",
    "from torch.utils.data.dataset import Subset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import time \n",
    "import wandb\n",
    "from data import ClusteredDataset_inturn, StructureDataset, batch_collate_function_nocluster\n",
    "\n",
    "import TPFold \n",
    "import nolgfold\n",
    "import utils\n",
    "import torch.distributed as dist\n",
    "import torch.optim as optim\n",
    "from openfold.utils.rigid_utils import Rigid\n",
    "from openfold.utils.loss import compute_fape\n",
    "import noam_opt\n",
    "import omegaconf\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--shuffle', type=float, default=0., help='Shuffle fraction')\n",
    "parser.add_argument('--data_jsonl', type=str, default=\"/pubhome/xtzhang/data/test_set.jsonl\", help='Path for the jsonl data')\n",
    "parser.add_argument('--split_json', type=str, default=\"/pubhome/bozhang/data/tmpnn_v8.json\",help='Path for the split json file')\n",
    "parser.add_argument('--output_folder',type=str,default=\"/pubhome/xtzhang/result/output/\",help=\"output folder for the log files and model parameters\")\n",
    "parser.add_argument('--save_folder',type=str,default=\"/pubhome/xtzhang/result/save/\",help=\"output folder for the model parameters\")\n",
    "parser.add_argument('--description',type=str,help=\"description the model information into wandb\")\n",
    "parser.add_argument('--job_name',type=str,default=\"noplm_eva\",help=\"jobname of the wandb dashboard\")\n",
    "parser.add_argument('--num_tags',type=int,default=6,help=\"num tags for the sequence\")\n",
    "parser.add_argument('--epochs',type=int,default=5,help=\"epochs to train the model\")\n",
    "parser.add_argument('--batch_size',type=int,default=1,help=\"batch size tokens\")\n",
    "parser.add_argument('--max_length',type=int,default=800,help=\"max length of the training sequence\")\n",
    "parser.add_argument('--max_tokens',type=int,default=400,help=\"max length of the training sequence\")\n",
    "parser.add_argument('--mask',type=float,default=1.0,help=\"mask fractions into input sequences\")\n",
    "parser.add_argument(\"--local_rank\", default=0, help=\"local device ID\", type=int) \n",
    "parser.add_argument('--parameters',type=str,default=\"/pubhome/xtzhang/result/save/no_plm_or_else_384epoch4.pt\", help=\"parameters path\")\n",
    "parser.add_argument('--lr',type=float,default=5e-4, help=\"learning rate of Adam optimizer\")\n",
    "parser.add_argument('--chunk_size',type=int,default=4,help=\"chunk size of the model\")\n",
    "parser.add_argument('--world_size',type=int,default=2,help=\"world_size\")\n",
    "parser.add_argument('--pattern',type=str,default=\"no\",help=\"mode\")\n",
    "parser.add_argument('--add_tmbed',type=bool,default=False, help=\"whether addtmbed\")\n",
    "parser.add_argument('--watch_freq',type=int,default=500, help=\"watch gradient\")\n",
    "# parser.add_argument('--pdb',default=\"/pubhome/xtzhang/output/pdb/800aa_noseq_nocctop\", help=\"Path to output PDB directory\", type=Path, required=True)\n",
    "# parser.add_argument(\"-o\", \"--pdb\", help=\"Path to output PDB directory\", type=Path, required=True)\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESMFold(\n",
       "  (tmbed_model): tmbed2(\n",
       "    (encoder_model): T5EncoderModel(\n",
       "      (shared): Embedding(128, 1024)\n",
       "      (encoder): T5Stack(\n",
       "        (embed_tokens): Embedding(128, 1024)\n",
       "        (block): ModuleList(\n",
       "          (0): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (relative_attention_bias): Embedding(32, 32)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (4): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (5): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (6): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (7): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (8): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (9): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (10): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (11): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (12): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (13): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (14): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (15): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (16): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (17): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (18): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (19): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (20): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (21): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (22): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (23): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): T5LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (encoder): T5EncoderModel(\n",
       "      (shared): Embedding(128, 1024)\n",
       "      (encoder): T5Stack(\n",
       "        (embed_tokens): Embedding(128, 1024)\n",
       "        (block): ModuleList(\n",
       "          (0): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (relative_attention_bias): Embedding(32, 32)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (4): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (5): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (6): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (7): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (8): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (9): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (10): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (11): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (12): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (13): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (14): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (15): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (16): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (17): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (18): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (19): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (20): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (21): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (22): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (23): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (k): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (v): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                  (o): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=1024, out_features=16384, bias=False)\n",
       "                  (wo): Linear(in_features=16384, out_features=1024, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): T5LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (model0): Predictor(\n",
       "      (model): CNN(\n",
       "        (input): Conv(\n",
       "          (func): ReLU(inplace=True)\n",
       "          (norm): SeqNorm()\n",
       "          (conv): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (dwc1): Conv(\n",
       "          (func): ReLU(inplace=True)\n",
       "          (norm): SeqNorm()\n",
       "          (conv): Conv2d(64, 64, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=64, bias=False)\n",
       "        )\n",
       "        (dwc2): Conv(\n",
       "          (func): ReLU(inplace=True)\n",
       "          (norm): SeqNorm()\n",
       "          (conv): Conv2d(64, 64, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=64, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout2d(p=0.5, inplace=True)\n",
       "        (output): Conv2d(192, 5, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (model1): Predictor(\n",
       "      (model): CNN(\n",
       "        (input): Conv(\n",
       "          (func): ReLU(inplace=True)\n",
       "          (norm): SeqNorm()\n",
       "          (conv): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (dwc1): Conv(\n",
       "          (func): ReLU(inplace=True)\n",
       "          (norm): SeqNorm()\n",
       "          (conv): Conv2d(64, 64, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=64, bias=False)\n",
       "        )\n",
       "        (dwc2): Conv(\n",
       "          (func): ReLU(inplace=True)\n",
       "          (norm): SeqNorm()\n",
       "          (conv): Conv2d(64, 64, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=64, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout2d(p=0.5, inplace=True)\n",
       "        (output): Conv2d(192, 5, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (model2): Predictor(\n",
       "      (model): CNN(\n",
       "        (input): Conv(\n",
       "          (func): ReLU(inplace=True)\n",
       "          (norm): SeqNorm()\n",
       "          (conv): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (dwc1): Conv(\n",
       "          (func): ReLU(inplace=True)\n",
       "          (norm): SeqNorm()\n",
       "          (conv): Conv2d(64, 64, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=64, bias=False)\n",
       "        )\n",
       "        (dwc2): Conv(\n",
       "          (func): ReLU(inplace=True)\n",
       "          (norm): SeqNorm()\n",
       "          (conv): Conv2d(64, 64, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=64, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout2d(p=0.5, inplace=True)\n",
       "        (output): Conv2d(192, 5, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (model3): Predictor(\n",
       "      (model): CNN(\n",
       "        (input): Conv(\n",
       "          (func): ReLU(inplace=True)\n",
       "          (norm): SeqNorm()\n",
       "          (conv): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (dwc1): Conv(\n",
       "          (func): ReLU(inplace=True)\n",
       "          (norm): SeqNorm()\n",
       "          (conv): Conv2d(64, 64, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=64, bias=False)\n",
       "        )\n",
       "        (dwc2): Conv(\n",
       "          (func): ReLU(inplace=True)\n",
       "          (norm): SeqNorm()\n",
       "          (conv): Conv2d(64, 64, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=64, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout2d(p=0.5, inplace=True)\n",
       "        (output): Conv2d(192, 5, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (model4): Predictor(\n",
       "      (model): CNN(\n",
       "        (input): Conv(\n",
       "          (func): ReLU(inplace=True)\n",
       "          (norm): SeqNorm()\n",
       "          (conv): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (dwc1): Conv(\n",
       "          (func): ReLU(inplace=True)\n",
       "          (norm): SeqNorm()\n",
       "          (conv): Conv2d(64, 64, kernel_size=(9, 1), stride=(1, 1), padding=(4, 0), groups=64, bias=False)\n",
       "        )\n",
       "        (dwc2): Conv(\n",
       "          (func): ReLU(inplace=True)\n",
       "          (norm): SeqNorm()\n",
       "          (conv): Conv2d(64, 64, kernel_size=(21, 1), stride=(1, 1), padding=(10, 0), groups=64, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout2d(p=0.5, inplace=True)\n",
       "        (output): Conv2d(192, 5, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (crf): CRF(num_tags=6)\n",
       "  (esm_s_mlp): Sequential(\n",
       "    (0): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=2560, out_features=1024, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (tm_s_mlp): Sequential(\n",
       "    (0): Linear(in_features=5, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (embedding): Embedding(23, 1024, padding_idx=0)\n",
       "  (trunk): FoldingTrunk(\n",
       "    (pairwise_positional_embedding): RelativePosition(\n",
       "      (embedding): Embedding(66, 128)\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (18): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (19): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (20): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (21): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (22): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (23): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (24): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (25): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (26): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (27): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (28): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (29): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (30): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (31): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (32): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (33): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (34): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (35): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (36): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (37): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (38): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (39): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (40): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (41): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (42): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (43): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (44): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (45): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (46): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (47): TriangularSelfAttentionBlock(\n",
       "        (layernorm_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (sequence_to_pair): SequenceToPair(\n",
       "          (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (proj): Linear(in_features=1024, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (pair_to_sequence): PairToSequence(\n",
       "          (layernorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "        )\n",
       "        (seq_attention): Attention(\n",
       "          (proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (g_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (tri_mul_out): TriangleMultiplicationOutgoing(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_mul_in): TriangleMultiplicationIncoming(\n",
       "          (linear_a_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_a_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_p): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_b_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (linear_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (layer_norm_in): LayerNorm()\n",
       "          (layer_norm_out): LayerNorm()\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (tri_att_start): TriangleAttention(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (tri_att_end): TriangleAttentionEndingNode(\n",
       "          (layer_norm): LayerNorm()\n",
       "          (linear): Linear(in_features=128, out_features=4, bias=False)\n",
       "          (mha): Attention(\n",
       "            (linear_q): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_k): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_v): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (linear_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_g): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sigmoid): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (mlp_seq): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp_pair): ResidueMLP(\n",
       "          (mlp): Sequential(\n",
       "            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "        (row_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "        (col_drop): Dropout(\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (recycle_s_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (recycle_z_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (recycle_disto): Embedding(15, 128)\n",
       "    (structure_module): StructureModule(\n",
       "      (layer_norm_s): LayerNorm()\n",
       "      (layer_norm_z): LayerNorm()\n",
       "      (linear_in): Linear(in_features=384, out_features=384, bias=True)\n",
       "      (ipa): InvariantPointAttention(\n",
       "        (linear_q): Linear(in_features=384, out_features=192, bias=True)\n",
       "        (linear_kv): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (linear_q_points): Linear(in_features=384, out_features=144, bias=True)\n",
       "        (linear_kv_points): Linear(in_features=384, out_features=432, bias=True)\n",
       "        (linear_b): Linear(in_features=128, out_features=12, bias=True)\n",
       "        (linear_out): Linear(in_features=2112, out_features=384, bias=True)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (softplus): Softplus(beta=1, threshold=20)\n",
       "      )\n",
       "      (ipa_dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm_ipa): LayerNorm()\n",
       "      (transition): StructureModuleTransition(\n",
       "        (layers): ModuleList(\n",
       "          (0): StructureModuleTransitionLayer(\n",
       "            (linear_1): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (linear_2): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (linear_3): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm): LayerNorm()\n",
       "      )\n",
       "      (bb_update): BackboneUpdate(\n",
       "        (linear): Linear(in_features=384, out_features=6, bias=True)\n",
       "      )\n",
       "      (angle_resnet): AngleResnet(\n",
       "        (linear_in): Linear(in_features=384, out_features=128, bias=True)\n",
       "        (linear_initial): Linear(in_features=384, out_features=128, bias=True)\n",
       "        (layers): ModuleList(\n",
       "          (0): AngleResnetBlock(\n",
       "            (linear_1): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_2): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "          (1): AngleResnetBlock(\n",
       "            (linear_1): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (linear_2): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (relu): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (linear_out): Linear(in_features=128, out_features=14, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (trunk2sm_s): Linear(in_features=1024, out_features=384, bias=True)\n",
       "    (trunk2sm_z): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (cctop_act): GELU(approximate='none')\n",
       "    (cctop_W): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (cctop_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (cctop_W_out): Linear(in_features=1024, out_features=6, bias=True)\n",
       "  )\n",
       "  (distogram_head): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (ptm_head): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (lm_head): Linear(in_features=1024, out_features=23, bias=True)\n",
       "  (lddt_head): Sequential(\n",
       "    (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=384, out_features=128, bias=True)\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): Linear(in_features=128, out_features=1850, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cfg=omegaconf.dictconfig.DictConfig( \n",
    " content={'_name': 'ESMFoldConfig', 'esm_type': 'esm2_3B', 'fp16_esm': True, 'use_esm_attn_map': False, 'esm_ablate_pairwise': False, 'esm_ablate_sequence': False, 'esm_input_dropout': 0, 'trunk': {'_name': 'FoldingTrunkConfig', 'num_blocks': 48, 'sequence_state_dim': 1024, 'pairwise_state_dim': 128, 'sequence_head_width': 32, 'pairwise_head_width': 32, 'position_bins': 32, 'dropout': 0, 'layer_drop': 0, 'cpu_grad_checkpoint': False, 'max_recycles': 4, 'chunk_size': None, 'structure_module': {'c_s': 384, 'c_z': 128, 'c_ipa': 16, 'c_resnet': 128, 'no_heads_ipa': 12, 'no_qk_points': 4, 'no_v_points': 8, 'dropout_rate': 0.1, 'no_blocks': 8, 'no_transition_layers': 1, 'no_resnet_blocks': 2, 'no_angles': 7, 'trans_scale_factor': 10, 'epsilon': 1e-08, 'inf': 100000.0}}, 'embed_aa': True, 'bypass_lm': False, 'lddt_head_hid_dim': 128}\n",
    " )\n",
    "model_no = nolgfold.load_model(chunk_size=64, pattern=args.pattern, model_path=\"/pubhome/xtzhang/result/save/no_plm_or_else_384epoch4.pt\", cfg=cfg)\n",
    "model_no = model_no.to(device)\n",
    "model_no.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m TPFold\u001b[39m.\u001b[39;49mload_model(chunk_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m, pattern\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mpattern, model_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m/pubhome/xtzhang/result/save/800aa_noseq_nocctopepoch4.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m, cfg\u001b[39m=\u001b[39;49mcfg)\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m~/myesm/TPFold.py:444\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(chunk_size, model_path, cfg, pattern, num_tags, add_tmbed)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_model\u001b[39m(chunk_size, model_path, cfg, pattern\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mno\u001b[39m\u001b[39m'\u001b[39m, num_tags\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, add_tmbed\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    442\u001b[0m     model_data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(\u001b[39mstr\u001b[39m(model_path), map_location\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m#pickledict\u001b[39;00m\n\u001b[0;32m--> 444\u001b[0m     model \u001b[39m=\u001b[39m ESMFold(pattern\u001b[39m=\u001b[39;49mpattern, num_tags\u001b[39m=\u001b[39;49mnum_tags, add_tmbed\u001b[39m=\u001b[39;49madd_tmbed, esmfold_config\u001b[39m=\u001b[39;49mcfg) \u001b[39m# make an instance\u001b[39;00m\n\u001b[1;32m    445\u001b[0m     model_state \u001b[39m=\u001b[39m model_data[\u001b[39m\"\u001b[39m\u001b[39mmodel_state_dict\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    446\u001b[0m     model\u001b[39m.\u001b[39mload_state_dict(model_state, strict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/myesm/TPFold.py:45\u001b[0m, in \u001b[0;36mESMFold.__init__\u001b[0;34m(self, pattern, esmfold_config, num_tags, add_tmbed, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_tmbed\u001b[39m=\u001b[39madd_tmbed\n\u001b[1;32m     42\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtmbed_model \u001b[39m=\u001b[39m tmbed2(use_gpu\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 45\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mesm, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mesm_dict \u001b[39m=\u001b[39m esm\u001b[39m.\u001b[39;49mpretrained\u001b[39m.\u001b[39;49mesm2_t36_3B_UR50D()\n\u001b[1;32m     46\u001b[0m \u001b[39m# self.esm_dict = esm.data.Alphabet.from_architecture(\"ESM-1b\")\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mesm\u001b[39m.\u001b[39mrequires_grad_(\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/myesm/esm/pretrained.py:387\u001b[0m, in \u001b[0;36mesm2_t36_3B_UR50D\u001b[0;34m()\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mesm2_t36_3B_UR50D\u001b[39m():\n\u001b[1;32m    383\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"36 layer ESM-2 model with 3B params, trained on UniRef50.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \n\u001b[1;32m    385\u001b[0m \u001b[39m    Returns a tuple of (Model, Alphabet).\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 387\u001b[0m     \u001b[39mreturn\u001b[39;00m load_model_and_alphabet_hub(\u001b[39m\"\u001b[39;49m\u001b[39mesm2_t36_3B_UR50D\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/myesm/esm/pretrained.py:64\u001b[0m, in \u001b[0;36mload_model_and_alphabet_hub\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_model_and_alphabet_hub\u001b[39m(model_name):\n\u001b[1;32m     63\u001b[0m     model_data, regression_data \u001b[39m=\u001b[39m _download_model_and_regression_data(model_name)\n\u001b[0;32m---> 64\u001b[0m     \u001b[39mreturn\u001b[39;00m load_model_and_alphabet_core(model_name, model_data, regression_data)\n",
      "File \u001b[0;32m~/myesm/esm/pretrained.py:191\u001b[0m, in \u001b[0;36mload_model_and_alphabet_core\u001b[0;34m(model_name, model_data, regression_data)\u001b[0m\n\u001b[1;32m    188\u001b[0m     model_data[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mupdate(regression_data[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    190\u001b[0m \u001b[39mif\u001b[39;00m model_name\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mesm2\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 191\u001b[0m     model, alphabet, model_state \u001b[39m=\u001b[39m _load_model_and_alphabet_core_v2(model_data)\n\u001b[1;32m    192\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m     model, alphabet, model_state \u001b[39m=\u001b[39m _load_model_and_alphabet_core_v1(model_data)\n",
      "File \u001b[0;32m~/myesm/esm/pretrained.py:176\u001b[0m, in \u001b[0;36m_load_model_and_alphabet_core_v2\u001b[0;34m(model_data)\u001b[0m\n\u001b[1;32m    174\u001b[0m state_dict \u001b[39m=\u001b[39m upgrade_state_dict(state_dict)\n\u001b[1;32m    175\u001b[0m alphabet \u001b[39m=\u001b[39m esm\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mAlphabet\u001b[39m.\u001b[39mfrom_architecture(\u001b[39m\"\u001b[39m\u001b[39mESM-1b\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 176\u001b[0m model \u001b[39m=\u001b[39m ESM2(\n\u001b[1;32m    177\u001b[0m     num_layers\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mencoder_layers,\n\u001b[1;32m    178\u001b[0m     embed_dim\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mencoder_embed_dim,\n\u001b[1;32m    179\u001b[0m     attention_heads\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mencoder_attention_heads,\n\u001b[1;32m    180\u001b[0m     alphabet\u001b[39m=\u001b[39;49malphabet,\n\u001b[1;32m    181\u001b[0m     token_dropout\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mtoken_dropout,\n\u001b[1;32m    182\u001b[0m )\n\u001b[1;32m    183\u001b[0m \u001b[39mreturn\u001b[39;00m model, alphabet, state_dict\n",
      "File \u001b[0;32m~/myesm/esm/model/esm2.py:39\u001b[0m, in \u001b[0;36mESM2.__init__\u001b[0;34m(self, num_layers, embed_dim, attention_heads, alphabet, token_dropout)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mappend_eos \u001b[39m=\u001b[39m alphabet\u001b[39m.\u001b[39mappend_eos\n\u001b[1;32m     37\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_dropout \u001b[39m=\u001b[39m token_dropout\n\u001b[0;32m---> 39\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_submodules()\n",
      "File \u001b[0;32m~/myesm/esm/model/esm2.py:50\u001b[0m, in \u001b[0;36mESM2._init_submodules\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_scale \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     43\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(\n\u001b[1;32m     44\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malphabet_size,\n\u001b[1;32m     45\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim,\n\u001b[1;32m     46\u001b[0m     padding_idx\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_idx,\n\u001b[1;32m     47\u001b[0m )\n\u001b[1;32m     49\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList(\n\u001b[0;32m---> 50\u001b[0m     [\n\u001b[1;32m     51\u001b[0m         TransformerLayer(\n\u001b[1;32m     52\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim,\n\u001b[1;32m     53\u001b[0m             \u001b[39m4\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim,\n\u001b[1;32m     54\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_heads,\n\u001b[1;32m     55\u001b[0m             add_bias_kv\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     56\u001b[0m             use_esm1b_layer_norm\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     57\u001b[0m             use_rotary_embeddings\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     58\u001b[0m         )\n\u001b[1;32m     59\u001b[0m         \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers)\n\u001b[1;32m     60\u001b[0m     ]\n\u001b[1;32m     61\u001b[0m )\n\u001b[1;32m     63\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontact_head \u001b[39m=\u001b[39m ContactPredictionHead(\n\u001b[1;32m     64\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_heads,\n\u001b[1;32m     65\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepend_bos,\n\u001b[1;32m     66\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mappend_eos,\n\u001b[1;32m     67\u001b[0m     eos_idx\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39meos_idx,\n\u001b[1;32m     68\u001b[0m )\n\u001b[1;32m     69\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb_layer_norm_after \u001b[39m=\u001b[39m ESM1bLayerNorm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim)\n",
      "File \u001b[0;32m~/myesm/esm/model/esm2.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_scale \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     43\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(\n\u001b[1;32m     44\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malphabet_size,\n\u001b[1;32m     45\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim,\n\u001b[1;32m     46\u001b[0m     padding_idx\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_idx,\n\u001b[1;32m     47\u001b[0m )\n\u001b[1;32m     49\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList(\n\u001b[1;32m     50\u001b[0m     [\n\u001b[0;32m---> 51\u001b[0m         TransformerLayer(\n\u001b[1;32m     52\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim,\n\u001b[1;32m     53\u001b[0m             \u001b[39m4\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim,\n\u001b[1;32m     54\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention_heads,\n\u001b[1;32m     55\u001b[0m             add_bias_kv\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     56\u001b[0m             use_esm1b_layer_norm\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     57\u001b[0m             use_rotary_embeddings\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     58\u001b[0m         )\n\u001b[1;32m     59\u001b[0m         \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers)\n\u001b[1;32m     60\u001b[0m     ]\n\u001b[1;32m     61\u001b[0m )\n\u001b[1;32m     63\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontact_head \u001b[39m=\u001b[39m ContactPredictionHead(\n\u001b[1;32m     64\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_heads,\n\u001b[1;32m     65\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepend_bos,\n\u001b[1;32m     66\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mappend_eos,\n\u001b[1;32m     67\u001b[0m     eos_idx\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39meos_idx,\n\u001b[1;32m     68\u001b[0m )\n\u001b[1;32m     69\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb_layer_norm_after \u001b[39m=\u001b[39m ESM1bLayerNorm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim)\n",
      "File \u001b[0;32m~/myesm/esm/modules.py:101\u001b[0m, in \u001b[0;36mTransformerLayer.__init__\u001b[0;34m(self, embed_dim, ffn_embed_dim, attention_heads, add_bias_kv, use_esm1b_layer_norm, use_rotary_embeddings)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_heads \u001b[39m=\u001b[39m attention_heads\n\u001b[1;32m    100\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_rotary_embeddings \u001b[39m=\u001b[39m use_rotary_embeddings\n\u001b[0;32m--> 101\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_submodules(add_bias_kv, use_esm1b_layer_norm)\n",
      "File \u001b[0;32m~/myesm/esm/modules.py:116\u001b[0m, in \u001b[0;36mTransformerLayer._init_submodules\u001b[0;34m(self, add_bias_kv, use_esm1b_layer_norm)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn_layer_norm \u001b[39m=\u001b[39m BertLayerNorm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim)\n\u001b[1;32m    115\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffn_embed_dim)\n\u001b[0;32m--> 116\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mLinear(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mffn_embed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim)\n\u001b[1;32m    118\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_layer_norm \u001b[39m=\u001b[39m BertLayerNorm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim)\n",
      "File \u001b[0;32m~/anaconda3/envs/TMprotein_predict/lib/python3.8/site-packages/torch/nn/modules/linear.py:101\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_parameter(\u001b[39m'\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreset_parameters()\n",
      "File \u001b[0;32m~/anaconda3/envs/TMprotein_predict/lib/python3.8/site-packages/torch/nn/modules/linear.py:107\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset_parameters\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[39m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[39m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[39m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     init\u001b[39m.\u001b[39;49mkaiming_uniform_(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, a\u001b[39m=\u001b[39;49mmath\u001b[39m.\u001b[39;49msqrt(\u001b[39m5\u001b[39;49m))\n\u001b[1;32m    108\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         fan_in, _ \u001b[39m=\u001b[39m init\u001b[39m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight)\n",
      "File \u001b[0;32m~/anaconda3/envs/TMprotein_predict/lib/python3.8/site-packages/torch/nn/init.py:412\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    410\u001b[0m bound \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39msqrt(\u001b[39m3.0\u001b[39m) \u001b[39m*\u001b[39m std  \u001b[39m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 412\u001b[0m     \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49muniform_(\u001b[39m-\u001b[39;49mbound, bound)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = TPFold.load_model(chunk_size=64, pattern=args.pattern, model_path=\"/pubhome/xtzhang/result/save/800aa_noseq_nocctopepoch4.pt\", cfg=cfg)\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK token:0,too long:0, 'not_match':1\n"
     ]
    }
   ],
   "source": [
    "jsonl_file = args.data_jsonl\n",
    "dataset = StructureDataset(jsonl_file=jsonl_file, max_length=args.max_length) \n",
    "test_loader = DataLoader(\n",
    "    dataset=dataset, \n",
    "    batch_size=args.batch_size, \n",
    "    collate_fn=batch_collate_function_nocluster)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for iteration, batch in enumerate(test_loader):            \n",
    "                for key in batch:\n",
    "                    batch[key] = batch[key].cuda(args.local_rank)\n",
    "                C_pos, CA_pos, N_pos, seq, mask, residx, bb_pos = batch['C_pos'], batch['CA_pos'], batch['N_pos'],batch['seq'], batch['mask'], batch['residx'], batch['bb_pos']                \n",
    "                output_dict = model(aa=seq, mask=mask, residx=residx)\n",
    "                output_dict_no = model_no(aa=seq, mask=mask, residx=residx)\n",
    "                target_frames = Rigid.from_3_points(C_pos, CA_pos, N_pos)\n",
    "\n",
    "                loss_fape = torch.mean(compute_fape(\n",
    "                                pred_frames=output_dict['pred_frames'],\n",
    "                                target_frames=target_frames,\n",
    "                                frames_mask=output_dict['frame_mask'],\n",
    "                                pred_positions=output_dict['backbone_positions'],\n",
    "                                target_positions=bb_pos,\n",
    "                                positions_mask=output_dict['backbone_atoms_mask'],\n",
    "                                length_scale=10,\n",
    "                            ))\n",
    "                loss_fape_no = torch.mean(compute_fape(\n",
    "                                pred_frames=output_dict_no['pred_frames'],\n",
    "                                target_frames=target_frames,\n",
    "                                frames_mask=output_dict_no['frame_mask'],\n",
    "                                pred_positions=output_dict_no['backbone_positions'],\n",
    "                                target_positions=bb_pos,\n",
    "                                positions_mask=output_dict_no['backbone_atoms_mask'],\n",
    "                                length_scale=10,\n",
    "                            ))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TMprotein_predict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9eb0802f6c9ed3b5f56a7212215bf0ff49df5c6f2ae731aae09e7070f7112c1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
