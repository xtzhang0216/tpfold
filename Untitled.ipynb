{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92f9b917-4a68-45a2-84fa-f52851e41a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG']=\":4096:8\"\n",
    "import sys\n",
    "import typing as T\n",
    "from dataclasses import dataclass\n",
    "from tqdm import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from omegaconf import MISSING\n",
    "from openfold.data.data_transforms import make_atom14_masks\n",
    "from openfold.utils.rigid_utils import Rigid\n",
    "from openfold.np import residue_constants\n",
    "from openfold.utils.loss import compute_predicted_aligned_error, compute_tm, compute_fape\n",
    "from torch import nn\n",
    "from torch.nn import LayerNorm\n",
    "from pathlib import Path\n",
    "from esm.myattention import AttentionMap\n",
    "import esm\n",
    "from esm import Alphabet\n",
    "from esm.esmfold.v1.categorical_mixture import categorical_lddt\n",
    "from esm.esmfold.v1.trunk import FoldingTrunk, FoldingTrunkConfig\n",
    "from esm.esmfold.v1.misc import (\n",
    "    batch_encode_sequences,\n",
    "    collate_dense_tensors,\n",
    "    output_to_pdb,\n",
    ")\n",
    "from esm.data import read_fasta\n",
    "import argparse\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "        \"--num-recycles\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help=\"Number of recycles to run. Defaults to number used in training (4).\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--max-tokens-per-batch\",\n",
    "        type=int,\n",
    "        default=1024,\n",
    "        help=\"Maximum number of tokens per gpu forward-pass. This will group shorter sequences together \"\n",
    "        \"for batched prediction. Lowering this can help with out of memory issues, if these occur on \"\n",
    "        \"short sequences.\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--chunk-size\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help=\"Chunks axial attention computation to reduce memory usage from O(L^2) to O(L). \"\n",
    "        \"Equivalent to running a for loop over chunks of of each dimension. Lower values will \"\n",
    "        \"result in lower memory usage at the cost of speed. Recommended values: 128, 64, 32. \"\n",
    "        \"Default: None.\"\n",
    "    )\n",
    "parser.add_argument(\"--cpu-only\", help=\"CPU only\", action=\"store_true\")\n",
    "parser.add_argument(\n",
    "        \"--cpu-offload\", help=\"Enable CPU offloading\", action=\"store_true\"\n",
    "    )\n",
    "args = parser.parse_args(args=[])\n",
    "sys.path.append(sys.path[0][0:-6] +'/TMbed_fromxu/tmbed')\n",
    "from predict import tmbed2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "739b40bb-879b-4b56-905e-a1c2501903a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class ESMFoldConfig:\n",
    "    trunk: T.Any = FoldingTrunkConfig()\n",
    "    lddt_head_hid_dim: int = 128\n",
    "\n",
    "class ESMFold(nn.Module):\n",
    "    def __init__(self, esmfold_config=None, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cfg = esmfold_config if esmfold_config else ESMFoldConfig(**kwargs)\n",
    "        cfg = self.cfg\n",
    "\n",
    "        self.distogram_bins = 64\n",
    "        \n",
    "        self.tmbed_model = tmbed2(use_gpu=False)\n",
    "\n",
    "\n",
    "        self.esm, self.esm_dict = esm.pretrained.esm2_t36_3B_UR50D()\n",
    "\n",
    "        self.esm.requires_grad_(False)\n",
    "        self.esm.half() # ???\n",
    "        # self.esm_feats,2560; \n",
    "        self.esm_feats = self.esm.embed_dim\n",
    "        # self.esm_attns=1440; self.esm.num_layers=36; self.esm.attention_heads=40\n",
    "        self.esm_attns = self.esm.num_layers * self.esm.attention_heads\n",
    "        self.register_buffer(\"af2_to_esm\", ESMFold._af2_to_esm(self.esm_dict))\n",
    "        self.esm_s_combine = nn.Parameter(torch.zeros(self.esm.num_layers + 1))\n",
    "        # c_s=1024, c_z=128\n",
    "\n",
    "        c_s = cfg.trunk.sequence_state_dim\n",
    "        c_z = cfg.trunk.pairwise_state_dim\n",
    "\n",
    "        self.esm_s_mlp = nn.Sequential(\n",
    "            LayerNorm(self.esm_feats),\n",
    "            nn.Linear(self.esm_feats, c_s),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(c_s, c_s),\n",
    "        )\n",
    "        self.tm_dim = 5\n",
    "        # 3,65,5 -> 3,65,2560 -> 3,65,37*2560\n",
    "        self.tmbed2s = nn.Sequential(\n",
    "            # LayerNorm(self.esm_feats),\n",
    "            nn.Linear(self.tm_dim, self.esm_feats, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.esm_feats, (self.esm.num_layers+1)*self.esm_feats, bias=False),\n",
    "        )\n",
    "        self.tmbed2z = AttentionMap(self.tm_dim, self.esm.embed_dim, num_heads=128, gated=False)\n",
    "\n",
    "        # 0 is padding, N is unknown residues, N + 1 is mask.\n",
    "        self.n_tokens_embed = residue_constants.restype_num + 3\n",
    "        self.pad_idx = 0\n",
    "        self.unk_idx = self.n_tokens_embed - 2\n",
    "        self.mask_idx = self.n_tokens_embed - 1\n",
    "        self.embedding = nn.Embedding(self.n_tokens_embed, c_s, padding_idx=0)\n",
    "\n",
    "        self.trunk = FoldingTrunk(**cfg.trunk)\n",
    "\n",
    "        self.distogram_head = nn.Linear(c_z, self.distogram_bins)\n",
    "        self.ptm_head = nn.Linear(c_z, self.distogram_bins)\n",
    "        self.lm_head = nn.Linear(c_s, self.n_tokens_embed)\n",
    "        self.lddt_bins = 50\n",
    "        self.lddt_head = nn.Sequential(\n",
    "            nn.LayerNorm(cfg.trunk.structure_module.c_s),\n",
    "            nn.Linear(cfg.trunk.structure_module.c_s, cfg.lddt_head_hid_dim),\n",
    "            nn.Linear(cfg.lddt_head_hid_dim, cfg.lddt_head_hid_dim),\n",
    "            nn.Linear(cfg.lddt_head_hid_dim, 37 * self.lddt_bins),\n",
    "        )\n",
    "        self._froezen(patern='v1')\n",
    "\n",
    "    def _froezen(self,patern=\"v1\"):\n",
    "        \"\"\"\n",
    "        Only training the following part of the modules:\n",
    "        - ipa module\n",
    "        - bb updata\n",
    "        - transition\n",
    "        - \n",
    "        - \n",
    "        \"\"\"\n",
    "        if patern == \"v1\":\n",
    "            for name,parameter in self.named_parameters():\n",
    "                if name.startswith(\"tmbed2\"):\n",
    "                    parameter.requires_grad = True\n",
    "                # elif name.startswith(\"esm_s_\"):\n",
    "                #     parameter.requires_grad = True\n",
    "                # elif name.startswith(\"trunk.structure_module\"):\n",
    "                #     parameter.requires_grad = True\n",
    "                # elif name.startswith(\"embedding\"):\n",
    "                #     parameter.requires_grad = True\n",
    "                else:\n",
    "                    parameter.requires_grad = False\n",
    "\n",
    "    @staticmethod\n",
    "    def _af2_to_esm(d: Alphabet):\n",
    "        # Remember that t is shifted from residue_constants by 1 (0 is padding).\n",
    "        esm_reorder = [d.padding_idx] + [\n",
    "            d.get_idx(v) for v in residue_constants.restypes_with_x\n",
    "        ]\n",
    "        return torch.tensor(esm_reorder)\n",
    "\n",
    "    def _af2_idx_to_esm_idx(self, aa, mask):\n",
    "        aa = (aa + 1).masked_fill(mask != 1, 0)\n",
    "        return self.af2_to_esm[aa]\n",
    "\n",
    "    def _compute_language_model_representations(\n",
    "        self, esmaa: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Adds bos/eos tokens for the language model, since the structure module doesn't use these.\"\"\"\n",
    "        batch_size = esmaa.size(0)\n",
    "\n",
    "        bosi, eosi = self.esm_dict.cls_idx, self.esm_dict.eos_idx\n",
    "        bos = esmaa.new_full((batch_size, 1), bosi)\n",
    "        eos = esmaa.new_full((batch_size, 1), self.esm_dict.padding_idx)\n",
    "        esmaa = torch.cat([bos, esmaa, eos], dim=1)\n",
    "        # Use the first padding index as eos during inference.\n",
    "        esmaa[range(batch_size), (esmaa != 1).sum(1)] = eosi\n",
    "\n",
    "        res = self.esm(\n",
    "            esmaa,\n",
    "            repr_layers=range(self.esm.num_layers + 1),\n",
    "            need_head_weights=False,\n",
    "        )\n",
    "        esm_s = torch.stack(\n",
    "            [v for _, v in sorted(res[\"representations\"].items())], dim=2\n",
    "        )\n",
    "        esm_s = esm_s[:, 1:-1]  # B, L, nLayers, C\n",
    "        return esm_s\n",
    "\n",
    "    def _mask_inputs_to_esm(self, esmaa, pattern):\n",
    "        new_esmaa = esmaa.clone()\n",
    "        new_esmaa[pattern == 1] = self.esm_dict.mask_idx\n",
    "        return new_esmaa\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        aa: torch.Tensor,\n",
    "        mask: T.Optional[torch.Tensor] = None,\n",
    "        residx: T.Optional[torch.Tensor] = None,\n",
    "        masking_pattern: T.Optional[torch.Tensor] = None,\n",
    "        num_recycles: T.Optional[int] = None,\n",
    "        # seq= None,\n",
    "        \n",
    "    ):\n",
    "        \n",
    "        \"\"\"Runs a forward pass given input tokens. Use `model.infer` to\n",
    "        run inference from a sequence.\n",
    "\n",
    "        Args:\n",
    "            aa (torch.Tensor): Tensor containing indices corresponding to amino acids. Indices match\n",
    "                openfold.np.residue_constants.restype_order_with_x.\n",
    "            mask (torch.Tensor): Binary tensor with 1 meaning position is unmasked and 0 meaning position is masked.\n",
    "            residx (torch.Tensor): Residue indices of amino acids. Will assume contiguous if not provided.\n",
    "            masking_pattern (torch.Tensor): Optional masking to pass to the input. Binary tensor of the same size\n",
    "                as `aa`. Positions with 1 will be masked. ESMFold sometimes produces different samples when\n",
    "                different masks are provided.\n",
    "            num_recycles (int): How many recycle iterations to perform. If None, defaults to training max\n",
    "                recycles, which is 3.\n",
    "        \"\"\"  \n",
    "        \n",
    "        # if isinstance(sequences, str):\n",
    "        #     sequences = [sequences]\n",
    "\n",
    "        # aatype, mask, _residx, linker_mask, chain_index = batch_encode_sequences(\n",
    "        #     sequences, residue_index_offset, chain_linker\n",
    "        # )\n",
    "\n",
    "        # if residx is None:\n",
    "        #     residx = _residx\n",
    "        # elif not isinstance(residx, torch.Tensor):\n",
    "        #     residx = collate_dense_tensors(residx)\n",
    "\n",
    "        aa, mask, residx = map(\n",
    "            lambda x: x.to(self.device), (aa, mask, residx)\n",
    "        )\n",
    "        # aa = aatype\n",
    "        \n",
    "        # if mask is None:\n",
    "        #     mask = torch.ones_like(aa)\n",
    "\n",
    "        B = aa.shape[0] # batchsize\n",
    "        L = aa.shape[1] # L是全序列最大值,不足用0补充\n",
    "        device = aa.device\n",
    "\n",
    "        # if residx is None:\n",
    "        #     residx = torch.arange(L, device=device).expand_as(aa)\n",
    "\n",
    "        # === ESM ===\n",
    "        esmaa = self._af2_idx_to_esm_idx(aa, mask)\n",
    "\n",
    "        if masking_pattern is not None:\n",
    "                esmaa = self._mask_inputs_to_esm(esmaa, masking_pattern)\n",
    "        # esm_s应是语言模型抽取的embedding，预测个数×残基数×37×2560;\n",
    "        # 其中,第二维(dim=1维)的前len+1有数,其余mask为0.即[i,0:len+1,:,:]有数字???\n",
    "        # ???为什么会多一位非零位\n",
    "        esm_s = self._compute_language_model_representations(esmaa)\n",
    "        # esm_s = esm_s.detach()\n",
    "        # tmbed_s为B, ml, 5,其中[i,0:len,:,:]有值,其余mask\n",
    "        tmbed_s = self.tmbed_model(aatype=aa, mask=mask).to(device)\n",
    "        # tmbed_s = tmbed_s.detach()\n",
    "        esm_s += self.tmbed2s(tmbed_s).reshape(B, L, (self.esm.num_layers+1), self.esm.embed_dim)       # b,ml, 37*2560->b,ml,37,2560\n",
    "\n",
    "\n",
    "        # Convert esm_s to the precision used by the trunk and\n",
    "        # the structure module. These tensors may be a lower precision if, for example,\n",
    "        # we're running the language model in fp16 precision.类似于浮点类型转换。转换使plm于后续模块精度一致\n",
    "        esm_s = esm_s.to(self.esm_s_combine.dtype)\n",
    "\n",
    "        # esm_s = esm_s.detach()\n",
    "\n",
    "        # === preprocessing ===预测个数×残基数×2560;1×65×2560\n",
    "        # s为esm_s, layer_weights即esm_s_combine，=nn.Parameter(torch.zeros(self.esm.num_layers + 1))\n",
    "        # s = (softmax(layer_weights) * s).sum(0) esm_s_combine作为权重，将37层做加权和\n",
    "        esm_s = (self.esm_s_combine.softmax(0).unsqueeze(0) @ esm_s).squeeze(2)\n",
    "        # 1×65×1024\n",
    "        s_s_0 = self.esm_s_mlp(esm_s)\n",
    "        # 1×65×65×1024\n",
    "        s_z_0 = self.tmbed2z(tmbed_s, mask=mask)\n",
    "        # s_z_0 = s_s_0.new_zeros(B, L, L, self.cfg.trunk.pairwise_state_dim)\n",
    "        # esm生成序列信息加上embedding？\n",
    "        s_s_0 += self.embedding(aa)\n",
    "\n",
    "        structure: dict = self.trunk(\n",
    "            s_s_0, s_z_0, aa, residx, mask, no_recycles=num_recycles,\n",
    "        )\n",
    "        # Documenting what we expect:\n",
    "        structure = {\n",
    "            k: v\n",
    "            for k, v in structure.items()\n",
    "            if k\n",
    "            in [\n",
    "                \"s_z\",\n",
    "                \"s_s\",\n",
    "                \"frames\",\n",
    "                \"sidechain_frames\",\n",
    "                \"unnormalized_angles\",\n",
    "                \"angles\",\n",
    "                \"positions\",\n",
    "                \"states\",\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "        # 8,b,ml,14,3 -> 8,b,ml,3,3 -> \n",
    "        structure['backbone_positions'] = structure['positions'][..., :3, :]\n",
    "\n",
    "        # structure[\"s_z\"]为b,ml,ml,128 ->linear b,ml,ml,64 \n",
    "        disto_logits = self.distogram_head(structure[\"s_z\"])\n",
    "        disto_logits = (disto_logits + disto_logits.transpose(1, 2)) / 2\n",
    "        structure[\"distogram_logits\"] = disto_logits\n",
    "        # structure[\"s_s\"]为b,ml,1024->(linear) b,ml,23        \n",
    "        lm_logits = self.lm_head(structure[\"s_s\"])\n",
    "        structure[\"lm_logits\"] = lm_logits\n",
    "\n",
    "        structure[\"aatype\"] = aa # b,ml\n",
    "        make_atom14_masks(structure)\n",
    "\n",
    "        for k in [\n",
    "            \"atom14_atom_exists\", # b,ml,14, 为0，1值.表征14原子表示下，哪写原子存在(置为1\n",
    "            \"atom37_atom_exists\",  # b,ml,37\n",
    "        ]:\n",
    "            structure[k] *= mask.unsqueeze(-1)\n",
    "        structure[\"residue_index\"] = residx #b,nml\n",
    "        # structure[\"states\"]为8,b,lm,384->8,b,lm,37,50\n",
    "        lddt_head = self.lddt_head(structure[\"states\"]).reshape(\n",
    "            structure[\"states\"].shape[0], B, L, -1, self.lddt_bins\n",
    "        )\n",
    "        structure[\"lddt_head\"] = lddt_head \n",
    "        # 取lddt_head最后一维，其维度为(b,lm,37,50)生成plddt(b,ml,37)\n",
    "        plddt = categorical_lddt(lddt_head[-1], bins=self.lddt_bins)\n",
    "        structure[\"plddt\"] = 100 * plddt  # we predict plDDT between 0 and 1, scale to be between 0 and 100.\n",
    "        # structure[\"s_z\"]为b,ml,ml,128 ->linear b,ml,ml,64\n",
    "        ptm_logits = self.ptm_head(structure[\"s_z\"])\n",
    "\n",
    "        seqlen = mask.type(torch.int64).sum(1) #每个序列长度的tensor\n",
    "        structure[\"ptm_logits\"] = ptm_logits\n",
    "        structure[\"ptm\"] = torch.stack([\n",
    "            compute_tm(batch_ptm_logits[None, :sl, :sl], max_bins=31, no_bins=self.distogram_bins)\n",
    "            for batch_ptm_logits, sl in zip(ptm_logits, seqlen)\n",
    "        ]) # torch.Size([2])\n",
    "        # 增加了'aligned_confidence_probs', 'predicted_aligned_error', 'max_predicted_aligned_error'\n",
    "        structure.update(\n",
    "            compute_predicted_aligned_error(\n",
    "                ptm_logits, max_bin=31, no_bins=self.distogram_bins\n",
    "            )\n",
    "        )\n",
    "        # b,ml,37 * b,ml,1\n",
    "        # structure[\"atom37_atom_exists\"] = structure[\n",
    "        #     \"atom37_atom_exists\"\n",
    "        # ] * linker_mask.unsqueeze(2)\n",
    "        # plddt为b,ml,37, 求每个蛋白的全氨基酸全37原子的平均plddt,一个蛋白得到一个值\n",
    "        structure[\"mean_plddt\"] = (structure[\"plddt\"] * structure[\"atom37_atom_exists\"]).sum(\n",
    "            dim=(1, 2)\n",
    "        ) / structure[\"atom37_atom_exists\"].sum(dim=(1, 2))\n",
    "        # structure[\"chain_index\"] = chain_index\n",
    "        structure['frame_mask'] = mask\n",
    "        structure['backbone_atoms_mask'] = mask.repeat_interleave(3, dim=-1)\n",
    "\n",
    "        return structure\n",
    "\n",
    "    def output_to_pdb(self, output: T.Dict) -> T.List[str]:\n",
    "        \"\"\"Returns the pbd (file) string from the model given the model output.\"\"\"\n",
    "        return output_to_pdb(output)\n",
    "\n",
    "    def infer_pdbs(self, seqs: T.List[str], *args, **kwargs) -> T.List[str]:\n",
    "        \"\"\"Returns list of pdb (files) strings from the model given a list of input sequences.\"\"\"\n",
    "        output = self.infer(seqs, *args, **kwargs)\n",
    "        return self.output_to_pdb(output)\n",
    "\n",
    "    def infer_pdb(self, sequence: str, *args, **kwargs) -> str:\n",
    "        \"\"\"Returns the pdb (file) string from the model given an input sequence.\"\"\"\n",
    "        return self.infer_pdbs([sequence], *args, **kwargs)[0]\n",
    "\n",
    "    def set_chunk_size(self, chunk_size: T.Optional[int]):\n",
    "        # This parameter means the axial attention will be computed\n",
    "        # in a chunked manner. This should make the memory used more or less O(L) instead of O(L^2).\n",
    "        # It's equivalent to running a for loop over chunks of the dimension we're iterative over,\n",
    "        # where the chunk_size is the size of the chunks, so 128 would mean to parse 128-lengthed chunks.\n",
    "        # Setting the value to None will return to default behavior, disable chunking.\n",
    "        self.trunk.set_chunk_size(chunk_size)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.esm_s_combine.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcf1f61a-9437-402e-b314-6eddcd281d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_model(model_name):\n",
    "    if model_name.endswith(\".pt\"):  # local, treat as filepath\n",
    "        model_path = Path(model_name)\n",
    "        model_data = torch.load(str(model_path), map_location=\"cpu\")\n",
    "    else:  # load from hub\n",
    "        \n",
    "        url = f\"https://dl.fbaipublicfiles.com/fair-esm/models/{model_name}.pt\"\n",
    "        # urllib.request.urlretrieve(url, filename='esmfold_3B_v1.pt')\n",
    "        model_data = torch.hub.load_state_dict_from_url(url, progress=False, map_location=\"cpu\")\n",
    "\n",
    "    cfg = model_data[\"cfg\"][\"model\"]\n",
    "    model_state = model_data[\"model\"]\n",
    "    model = ESMFold(esmfold_config=cfg)\n",
    "\n",
    "    # expected_keys = set(model.state_dict().keys())\n",
    "    # found_keys = set(model_state.keys())\n",
    "\n",
    "    # missing_essential_keys = []\n",
    "    # for missing_key in expected_keys - found_keys:\n",
    "    #     if not missing_key.startswith(\"esm.\"):\n",
    "    #         missing_essential_keys.append(missing_key)\n",
    "\n",
    "    # if missing_essential_keys:\n",
    "    #     raise RuntimeError(f\"Keys '{', '.join(missing_essential_keys)}' are missing.\")\n",
    "\n",
    "    model.load_state_dict(model_state, strict=False)\n",
    "\n",
    "    return model\n",
    "\n",
    "def collate_fn(batch):\n",
    "    seqs, coords, lengths = [],[],[]\n",
    "    for batch_i in batch:\n",
    "        seqs.append(batch_i[0])\n",
    "        lengths.append(batch_i[1])\n",
    "        coords.append(batch_i[2])\n",
    "    aatype, mask, _residx, linker_mask, chain_index = batch_encode_sequences(sequences=seqs)\n",
    "    target_positions = generate_label_tensors(coords)\n",
    "    return aatype, mask, _residx, target_positions \n",
    "\n",
    "def generate_label_tensors(structures):\n",
    "    all_pos_list = []\n",
    "    for pos_dict in structures:\n",
    "        # cat的顺序不重要，但一定要保证pred和targ一致，才能比较距离\n",
    "        one_pos_tensor = torch.tensor(())\n",
    "        for pos_i in zip(pos_dict['N'], pos_dict['CA'], pos_dict['C']):\n",
    "            one_pos_tensor = torch.cat((one_pos_tensor, torch.tensor(pos_i)), dim=0)\n",
    "        all_pos_list.append(one_pos_tensor)\n",
    "\n",
    "    target_positions = collate_dense_tensors(all_pos_list)\n",
    "    return target_positions\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, file_path) -> None:\n",
    "        super().__init__()\n",
    "        with open(file_path, \"r\") as f:\n",
    "            examples = [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "        self.name, self.seq, self.coords, self.length = [], [], [], []\n",
    "        for example in examples:\n",
    "            self.name.append(example['name'])\n",
    "            self.seq.append(example['seq'])\n",
    "            self.coords.append(example['coords'])\n",
    "            self.length.append(example['length'])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.name)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.seq[index], self.length[index], self.coords[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f711b55a-7351-477b-88b8-13fce468524e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\"exp\" \"_vml_cpu\" not implemented for 'Half'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_54642/1434772342.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# model = esm.pretrained.esmfold_v1()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"esmfold_3B_v1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# net = model.eval()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_54642/1056352273.py\u001b[0m in \u001b[0;36m_load_model\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cfg\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmodel_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mESMFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mesmfold_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# expected_keys = set(model.state_dict().keys())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_54642/781859193.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, esmfold_config, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistogram_bins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtmbed_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmbed2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/run01/scv6707/zxt/TMbed_fromxu/tmbed/predict.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, use_gpu, cpu_fallback, out_format, embeddings_file)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;31m# models = [model.to('cuda') for model in models]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/run01/scv6707/zxt/TMbed_fromxu/tmbed/tmbed.py\u001b[0m in \u001b[0;36mload_models\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodel_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'*.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/run01/scv6707/zxt/TMbed_fromxu/tmbed/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, channels)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mfilter_kernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgaussian_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'filter_kernel'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilter_kernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/run01/scv6707/zxt/TMbed_fromxu/tmbed/utils.py\u001b[0m in \u001b[0;36mgaussian_kernel\u001b[0;34m(kernel_size, std)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgaussian_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     kernel = [gaussian(i - (kernel_size // 2), std)\n\u001b[0;32m---> 95\u001b[0;31m               for i in range(kernel_size)]\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/run01/scv6707/zxt/TMbed_fromxu/tmbed/utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgaussian_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     kernel = [gaussian(i - (kernel_size // 2), std)\n\u001b[0;32m---> 95\u001b[0;31m               for i in range(kernel_size)]\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/run01/scv6707/zxt/TMbed_fromxu/tmbed/utils.py\u001b[0m in \u001b[0;36mgaussian\u001b[0;34m(x, std)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"exp\" \"_vml_cpu\" not implemented for 'Half'"
     ]
    }
   ],
   "source": [
    "n_gpus=2\n",
    "BATCH_SIZE=2\n",
    "tm_dataset = ProteinDataset('/data/home/scv6707/run/zxt/data/TMfromzb/json/easy.jsonl')\n",
    "\n",
    "# model = esm.pretrained.esmfold_v1()\n",
    "model = _load_model(\"esmfold_3B_v1\")\n",
    "# net = model.eval()\n",
    "model = model.train()\n",
    "model.set_chunk_size(args.chunk_size)\n",
    "\n",
    "loss = compute_fape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa92a34a-9d5a-4535-b471-235bb96d8e4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_54642/1771809874.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maatype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mfinetune_esm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtm_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "def finetune_esm(model, loss, train_dataset, local_rank=None, device='cuda',  batch_size=1, num_epoch=1, lr=0.01):\n",
    "    model.cuda()\n",
    "    # model = nn.parallel.DistributedDataParallel(model.cuda(local_rank), device_ids=[local_rank])\n",
    "    # train_sampler = torch.utils.data.distributed.DistributedSampler(tm_dataset) \n",
    "    # train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, sampler=train_sampler)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "    for epoch in range(num_epoch):\n",
    "\n",
    "        print(\"——————第 {} 轮训练开始——————\".format(epoch + 1))\n",
    "        # 训练开始\n",
    "        # net.train()\n",
    "        train_acc = 0\n",
    "        for aatype, mask, residx, target_positions in tqdm(train_dataloader, desc='训练'):\n",
    "           \n",
    "           \n",
    "            aatype, mask, residx, target_positions = map(\n",
    "                lambda x: x.cuda(), (aatype, mask, residx, target_positions)\n",
    "            )\n",
    "            \n",
    "            output_dict = model(aa=aatype, mask=mask, residx=residx) #header放在哪\n",
    "\n",
    "            print(aatype.shape)\n",
    "finetune_esm(model, loss, train_dataset=tm_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a491e28-24f8-4806-ade7-c93ba1750f9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e92b287-b501-48bc-b9b0-b0d17f80fdc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esmfold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad452e1673bff0d10bd152774fd4798bb54599322b7a85a69b90d3c3e1be29eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
